import requests
from bs4 import BeautifulSoup
import datetime
import os
import sys
import urllib.parse

# --- é…ç½® ---
TOKEN = os.environ.get("PUSHPLUS_TOKEN")
PAGES_TO_SCRAPE = 3 # è®¾å®šä¸ºæŠ“å–å‰ 3 é¡µæœç´¢ç»“æœ
RESULTS_PER_PAGE = 10 

# --- å…³é”®è¯åº“å‡çº§ï¼šæ‰©å¤§è¦†ç›–èŒƒå›´ ---
TOPICS = {
    # æ‰©å¤§èŒƒå›´ï¼šåŠ å…¥å­¦ä¸šè§„åˆ’ã€æ‹›ç”Ÿæ”¿ç­–ç­‰
    "é«˜è€ƒ/ä¸­è€ƒæ•™è‚²": ["é«˜è€ƒ", "ä¸­è€ƒ", "æ•™è‚²æ”¹é©", "æ‹›ç”Ÿæ”¿ç­–", "è‡ªä¸»æ‹›ç”Ÿ", "å¿—æ„¿å¡«æŠ¥", "åˆ†æ•°çº¿é¢„æµ‹", "å­¦ä¸šè§„åˆ’"],
    
    # æ‰©å¤§èŒƒå›´ï¼šåŠ å…¥æ•™è‚²ç†å¿µã€æ€§æ ¼å…»æˆã€çˆ¶æ¯è¯¾å ‚ç­‰
    "å®¶åº­æ•™è‚²": ["å®¶åº­æ•™è‚²", "äº²å­å…³ç³»", "æ•™è‚²ç†å¿µ", "æƒ…å•†åŸ¹å…»", "æ€§æ ¼å…»æˆ", "çˆ¶æ¯è¯¾å ‚", "äº²å­æ²Ÿé€š"],
    
    # æ‰©å¤§èŒƒå›´ï¼šåŠ å…¥æ—¶é—´ç®¡ç†ã€ä¸“æ³¨åŠ›ã€å­¦ä¹ ä¹ æƒ¯ç­‰
    "æˆé•¿å­¦ä¹ ": ["å­¦ä¹ æ–¹æ³•", "é«˜æ•ˆå­¦ä¹ ", "æˆé•¿æ€ç»´", "è®°å¿†åŠ›è®­ç»ƒ", "æ—¶é—´ç®¡ç†", "å­¦ä¹ ä¹ æƒ¯", "ä¸“æ³¨åŠ›"],
}

# --- æ ¸å¿ƒåŠŸèƒ½ (ä¿æŒç¨³å®š) ---

def get_search_results(query):
    """
    å®é™…èšåˆï¼šè°ƒç”¨ç™¾åº¦æ–°é—»æœç´¢ï¼Œå¹¶é™åˆ¶æ—¶é—´èŒƒå›´åœ¨æœ€è¿‘ 7 å¤©å†…
    """
    print(f"Executing Baidu News search for: {query} (Depth: {PAGES_TO_SCRAPE} pages)")
    
    # ç™¾åº¦æ–°é—»æœç´¢ URL å‚æ•°: rtt=4 (æ–°é—»æ¨¡å¼), gpc=1&qdr=7 (æœ€è¿‘ 7 å¤©)
    base_url = "https://www.baidu.com/s?tn=news&rtt=4&gpc=1&qdr=7&wd="
    
    full_url = base_url + urllib.parse.quote(query) 
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 14; SM-S928B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.144 Mobile Safari/537.36'
    }
    all_results = []
    
    for page in range(PAGES_TO_SCRAPE):
        offset = page * RESULTS_PER_PAGE
        full_url_with_offset = f"{full_url}&pn={offset}"
        
        try:
            resp = requests.get(full_url_with_offset, headers=headers, timeout=15)
            resp.raise_for_status() 
            resp.encoding = 'utf-8'
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # æ£€æŸ¥æ˜¯å¦å‘½ä¸­åçˆ¬
            if len(resp.text) < 10000 and page > 0: 
                print(f"Baidu Search Blocked on page {page+1}. Stopping pagination.")
                break
                
            search_results = soup.find_all('div', class_='result') or soup.find_all('div', class_='c-container')
            
            if not search_results:
                break 
                
            for result in search_results:
                title_tag = result.find('a', target='_blank')
                source_tag = result.find('p', class_='c-author') or result.find('span', class_='c-info')
                
                if title_tag and title_tag.get('href'):
                    title = title_tag.get_text(strip=True)
                    link = title_tag.get('href')
                    source_info = source_tag.get_text(strip=True) if source_tag else 'æœªçŸ¥æ¥æº'
                    
                    if len(title) > 10 and link not in [r['link'] for r in all_results]:
                        all_results.append({
                            "title": title,
                            "link": link,
                            "source": source_info
                        })

        except Exception as e:
            print(f"Baidu Search Error on page {page+1} for query '{query}': {e}")
            break

    return all_results

def send_push(title, content):
    """å‘é€åˆ°å¾®ä¿¡"""
    if not TOKEN: sys.exit(1)
    url = 'http://www.pushplus.plus/send'
    data = {"token": TOKEN, "title": title, "content": content, "template": "markdown"}
    
    try:
        requests.post(url, json=data, timeout=15)
        print("Push successful.")
    except Exception as e:
        print(f"Push failed: {e}")

def main():
    report_title = f"å…¨ç½‘çƒ­ç‚¹è¿½è¸ª ({datetime.date.today().strftime('%Y-%m-%d')})"
    report_parts = [f"## ğŸ”¥ å…¨ç½‘çƒ­ç‚¹è¿½è¸ª - å…³é”®è¯æ‰©å±• (7 å¤©æ—¶æ•ˆ)", "---"]
    all_results_found = False

    for topic, keywords in TOPICS.items():
        query_keywords = ' OR '.join(keywords) # ä½¿ç”¨ OR è¿æ¥å…³é”®è¯ï¼Œå¢åŠ å‘½ä¸­ç‡
        
        # *** æœ€ç»ˆæŸ¥è¯¢ï¼šåŒ…å«æ ¸å¿ƒè¯é¢˜å’Œå¾®ä¿¡å…¬ä¼—å· ***
        # è¿™å°†è®©ç™¾åº¦ä¼˜å…ˆè¿”å›åŒ…å«è¿™äº›å…³é”®è¯çš„ç½‘é¡µå’Œå…¬ä¼—å·æ–‡ç« 
        query = f"æ•™è‚² ({query_keywords}) å¾®ä¿¡å…¬ä¼—å·" 
        
        results = get_search_results(query) 
        
        if results:
            all_results_found = True
            
            report_parts.append(f"### ğŸš€ {topic} - çƒ­é—¨è®¨è®º")
            report_parts.append(f"*(å…±å‘ç° {len(results)} æ¡ï¼Œå·²åŒ…å«å¾®ä¿¡å…¬ä¼—å·)*")

            for i, item in enumerate(results[:15]): # æ˜¾ç¤ºå‰ 15 æ¡
                report_parts.append(f"- [{item['title']}]({item['link']}) ({item['source']})")
                
            report_parts.append("\n")

    if not all_results_found:
        report_parts.append("ä»Šæ—¥æœªå‘ç°ç¬¦åˆæ‰€æœ‰ä¸»é¢˜çš„æ˜ç¡®çƒ­ç‚¹ã€‚å½“å‰ç­›é€‰ä¸ºæœ€è¿‘ 7 å¤©ã€‚")
        
    report_parts.append("---")
    report_parts.append("*ğŸ’¡ ç»“æœæ¥è‡ªç™¾åº¦æ–°é—»èšåˆ (æœ€è¿‘ 7 å¤©ï¼Œå…³é”®è¯å·²æ‰©å±•ï¼ŒåŒ…å«å¾®ä¿¡æº)ã€‚*")

    send_push(report_title, "\n".join(report_parts))

if __name__ == "__main__":
    main()
