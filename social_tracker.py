import requests
from bs4 import BeautifulSoup
import datetime
import os
import sys
import urllib.parse

# --- é…ç½® ---
TOKEN = os.environ.get("PUSHPLUS_TOKEN")
PAGES_TO_SCRAPE = 3 # è®¾å®šä¸ºæŠ“å–å‰ 3 é¡µæœç´¢ç»“æœ
RESULTS_PER_PAGE = 10 

# å››å¤§ä¸»é¢˜å…³é”®è¯
TOPICS = {
    "é«˜è€ƒ/ä¸­è€ƒæ•™è‚²": ["é«˜è€ƒ", "ä¸­è€ƒ", "å¿—æ„¿å¡«æŠ¥", "åˆ†æ•°çº¿", "å¼ºåŸºè®¡åˆ’"],
    "å®¶åº­æ•™è‚²": ["å®¶åº­æ•™è‚²", "äº²å­å…³ç³»", "æ•™è‚²æ–¹æ³•", "æƒ…å•†åŸ¹å…»"],
    "æˆé•¿å­¦ä¹ ": ["å­¦ä¹ æ–¹æ³•", "é«˜æ•ˆå­¦ä¹ ", "æˆé•¿æ€ç»´", "è®°å¿†åŠ›"],
}

# --- æ ¸å¿ƒåŠŸèƒ½ ---

def get_search_results(query):
    """
    å®é™…èšåˆï¼šè°ƒç”¨ç™¾åº¦æ–°é—»æœç´¢ï¼Œå¹¶é™åˆ¶æ—¶é—´èŒƒå›´åœ¨æœ€è¿‘ 7 å¤©å†…
    """
    print(f"Executing Baidu News search for: {query} (Depth: {PAGES_TO_SCRAPE} pages)")
    
    # ç™¾åº¦æ–°é—»æœç´¢ URL å‚æ•°
    # rtt=4 (æ–°é—»æ¨¡å¼), gpc=1&qdr=7 (æœ€è¿‘ 7 å¤©ï¼Œå·²æ›´æ–°!)
    base_url = "https://www.baidu.com/s?tn=news&rtt=4&gpc=1&qdr=7&wd="
    
    # ... (å…¶ä½™ä»£ç ä¿æŒä¸å˜) ...
    
    full_url = base_url + urllib.parse.quote(query) 
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 14; SM-S928B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.144 Mobile Safari/537.36'
    }
    all_results = []
    
    # --- æ ¸å¿ƒå‡çº§ï¼šåˆ†é¡µå¾ªç¯ ---
    for page in range(PAGES_TO_SCRAPE):
        offset = page * RESULTS_PER_PAGE # pn=0 (page 1), pn=10 (page 2), pn=20 (page 3)
        
        full_url_with_offset = f"{full_url}&pn={offset}"
        
        try:
            resp = requests.get(full_url_with_offset, headers=headers, timeout=15)
            resp.raise_for_status() 
            resp.encoding = 'utf-8'
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # æ£€æŸ¥æ˜¯å¦å‘½ä¸­åçˆ¬
            if len(resp.text) < 10000 and page > 0: 
                print(f"Baidu Search Blocked on page {page+1}. Stopping pagination.")
                break
                
            search_results = soup.find_all('div', class_='result') or soup.find_all('div', class_='c-container')
            
            if not search_results:
                break 
                
            for result in search_results:
                title_tag = result.find('a', target='_blank')
                source_tag = result.find('p', class_='c-author') or result.find('span', class_='c-info')
                
                if title_tag and title_tag.get('href'):
                    title = title_tag.get_text(strip=True)
                    link = title_tag.get('href')
                    source_info = source_tag.get_text(strip=True) if source_tag else 'æœªçŸ¥æ¥æº'
                    
                    if len(title) > 10 and link not in [r['link'] for r in all_results]: # é¿å…é‡å¤
                        all_results.append({
                            "title": title,
                            "link": link,
                            "source": source_info
                        })

        except Exception as e:
            print(f"Baidu Search Error on page {page+1} for query '{query}': {e}")
            break # å‡ºç°é”™è¯¯åˆ™åœæ­¢åˆ†é¡µ

    return all_results

def send_push(title, content):
    """å‘é€åˆ°å¾®ä¿¡"""
    if not TOKEN: 
        print("Error: PUSHPLUS_TOKEN missing.")
        sys.exit(1)
        
    url = 'http://www.pushplus.plus/send'
    data = {"token": TOKEN, "title": title, "content": content, "template": "markdown"}
    
    try:
        requests.post(url, json=data, timeout=15)
        print("Push successful.")
    except Exception as e:
        print(f"Push failed: {e}")

def main():
    report_title = f"å…¨ç½‘çƒ­ç‚¹è¿½è¸ª ({datetime.date.today().strftime('%Y-%m-%d')})"
    report_parts = [f"## ğŸ”¥ å…¨ç½‘çƒ­ç‚¹è¿½è¸ª - èšç„¦æŠ–éŸ³/å°çº¢ä¹¦ (7 å¤©æ—¶æ•ˆ)", "---"]
    all_results_found = False

    for topic, keywords in TOPICS.items():
        query_keywords = ' '.join(keywords) 
        
        # æ ¸å¿ƒæŸ¥è¯¢ï¼šå¼ºåˆ¶è¦æ±‚ç»“æœåŒ…å« 'å°çº¢ä¹¦' æˆ– 'æŠ–éŸ³'
        query = f"(å°çº¢ä¹¦ OR æŠ–éŸ³) AND (æ•™è‚² {query_keywords})" 
        
        results = get_search_results(query) 
        
        if results:
            all_results_found = True
            
            report_parts.append(f"### ğŸš€ {topic} - çƒ­é—¨è®¨è®º")
            report_parts.append(f"*(å…±å‘ç° {len(results)} æ¡ï¼Œå·²è¿‡æ»¤éæŠ–éŸ³/å°çº¢ä¹¦ç»“æœ)*")

            for i, item in enumerate(results[:15]): # æ˜¾ç¤ºå‰ 15 æ¡
                report_parts.append(f"- [{item['title']}]({item['link']}) ({item['source']})")
                
            report_parts.append("\n")

    if not all_results_found:
        report_parts.append("ä»Šæ—¥æœªå‘ç°ç¬¦åˆæ‰€æœ‰ä¸»é¢˜çš„æ˜ç¡®çƒ­ç‚¹ã€‚å½“å‰ç­›é€‰ä¸ºæœ€è¿‘ 7 å¤©ã€‚")
        
    report_parts.append("---")
    report_parts.append("*ğŸ’¡ ç»“æœæ¥è‡ªç™¾åº¦æ–°é—»èšåˆ (æœ€è¿‘ 7 å¤©ï¼Œèšç„¦å°çº¢ä¹¦/æŠ–éŸ³)ã€‚*")

    send_push(report_title, "\n".join(report_parts))

if __name__ == "__main__":
    main()
